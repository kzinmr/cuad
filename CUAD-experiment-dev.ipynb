{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd0625",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Python 3.8, PyTorch 1.7, and Transformers 4.3/4.4.\n",
    "# !pip install --upgrade pip\n",
    "# !pip install -qU torch==1.7.1\n",
    "# !pip install tensorboard==2.5.0 tensorflow==2.5.0 tensorflow-datasets==4.0.1\n",
    "!pip install -q transformers==4.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f747b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kzinmr/cuad.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e03ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd cuad && mkdir data && cp data.zip data/ && cd data && unzip ./data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47b823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd cuad && mkdir -p ./train_models/roberta-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98678d",
   "metadata": {},
   "source": [
    "# CUAD Dataset\n",
    "- in sufficient memory entironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "631e898a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['version', 'data'])\n",
      "22450\n",
      "['Document Name', 'Parties', 'Agreement Date', 'Effective Date', 'Expiration Date', 'Renewal Term', 'Notice Period to Terminate Renewal']\n",
      "4190\n"
     ]
    }
   ],
   "source": [
    "# filter by question type\n",
    "\n",
    "import json\n",
    "\n",
    "def filter_data(data, targets=['Parties']):\n",
    "    print(targets)\n",
    "    for d in data:\n",
    "        for p in d['paragraphs']:\n",
    "            p['qas'] = [qa for qa in p['qas'] if any(target in qa['question'] for target in targets)]\n",
    "\n",
    "# SquadV2Processor.get_train_examples を以下の絞り込みするように書き換える\n",
    "targets=['Document Name', 'Parties', 'Agreement Date', 'Effective Date', 'Expiration Date', 'Renewal Term','Notice Period to Terminate Renewal']\n",
    "with open('./data/train_separate_questions.json') as reader:\n",
    "    js = json.load(reader)\n",
    "    print(js.keys())\n",
    "    data = js[\"data\"]\n",
    "    version = js['version']\n",
    "    print(sum(len(p['qas']) for d in data for p in d['paragraphs']))\n",
    "    filter_data(data, targets)\n",
    "    print(sum(len(p['qas']) for d in data for p in d['paragraphs']))\n",
    "\n",
    "# (PARTIES)2012, (MASTER)4190\n",
    "with open('./data/train_separate_questions_master.json', 'w') as writer:\n",
    "    j = json.dumps({'data': data, 'version': version})\n",
    "    writer.write(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5727374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['version', 'data'])\n",
      "4182\n",
      "['Document Name', 'Parties', 'Agreement Date', 'Effective Date', 'Expiration Date', 'Renewal Term', 'Notice Period to Terminate Renewal']\n",
      "612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('./data/test.json') as reader:\n",
    "    js = json.load(reader)\n",
    "    print(js.keys())\n",
    "    data = js[\"data\"]\n",
    "    version = js['version']\n",
    "    print(sum(len(p['qas']) for d in data for p in d['paragraphs']))\n",
    "    filter_data(data, targets)\n",
    "    print(sum(len(p['qas']) for d in data for p in d['paragraphs']))\n",
    "\n",
    "# 102, 612\n",
    "with open('./data/test_master.json', 'w') as writer:\n",
    "    j = json.dumps({'data': data, 'version': version})\n",
    "    writer.write(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e10b103a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [01:35<00:00,  4.27it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4190"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "\n",
    "data_dir = './data'\n",
    "train_file = 'train_separate_questions_master.json'\n",
    "\n",
    "processor = SquadV2Processor()\n",
    "examples = processor.get_train_examples(data_dir, filename=train_file)\n",
    "len(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfe36ad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "convert squad examples to features: 100%|██████████| 4190/4190 [30:25<00:00,  2.30it/s] \n",
      "add example index and unique id: 100%|██████████| 4190/4190 [00:00<00:00, 22101.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 1min 35s, total: 2min 56s\n",
      "Wall time: 30min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    squad_convert_examples_to_features\n",
    ")\n",
    "output_dir = './train_models/roberta-base'\n",
    "model_name_or_path = 'roberta-base'\n",
    "tokenizer_name = model_name_or_path\n",
    "do_lower_case=False\n",
    "cache_dir=None\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_name,\n",
    "    do_lower_case=do_lower_case,\n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "evaluate=False\n",
    "threads=4\n",
    "max_seq_length=512\n",
    "max_query_length=256\n",
    "doc_stride=256\n",
    "\n",
    "features, dataset = squad_convert_examples_to_features(\n",
    "    examples=examples,\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=max_seq_length,\n",
    "    doc_stride=doc_stride,\n",
    "    max_query_length=max_query_length,\n",
    "    is_training=not evaluate,\n",
    "    return_dataset=\"pt\",\n",
    "    threads=threads,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15648f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "def get_dataset_pos_mask(dataset):\n",
    "    \"\"\"\n",
    "    Returns a list, pos_mask, where pos_mask[i] indicates is True if the ith example in the dataset is positive\n",
    "    (i.e. it contains some text that should be highlighted) and False otherwise.\n",
    "    \"\"\"\n",
    "    pos_mask = []\n",
    "    for i in range(len(dataset)):\n",
    "        ex = dataset[i]\n",
    "        start_pos = ex[3]\n",
    "        end_pos = ex[4]\n",
    "        is_positive = end_pos > start_pos\n",
    "        pos_mask.append(is_positive)\n",
    "    return pos_mask\n",
    "def get_balanced_dataset(dataset):\n",
    "    \"\"\"\n",
    "    returns a new dataset, where positive and negative examples are approximately balanced\n",
    "    \"\"\"\n",
    "    pos_mask = get_dataset_pos_mask(dataset)\n",
    "    neg_mask = [~mask for mask in pos_mask]\n",
    "    npos, nneg = np.sum(pos_mask), np.sum(neg_mask)\n",
    "\n",
    "    neg_keep_frac = npos / nneg  # So that in expectation there will be npos negative examples (--> balanced)\n",
    "    neg_keep_mask = [mask and np.random.random() < neg_keep_frac for mask in neg_mask]\n",
    "\n",
    "    # keep all positive examples and subset of negative examples\n",
    "    keep_mask = [pos_mask[i] or neg_keep_mask[i] for i in range(len(pos_mask))]\n",
    "    keep_indices = [i for i in range(len(keep_mask)) if keep_mask[i]]\n",
    "\n",
    "    subset_dataset = torch.utils.data.Subset(dataset, keep_indices)\n",
    "    return subset_dataset\n",
    "\n",
    "cache_dir = ''\n",
    "model_name_or_path = 'roberta-base'\n",
    "max_seq_length=512\n",
    "subset_cached_features_file = os.path.join(\n",
    "    cache_dir,\n",
    "    \"balanced_subset_cached_{}_{}_{}\".format(\n",
    "        \"dev\" if evaluate else \"train\",\n",
    "        list(filter(None, model_name_or_path.split(\"/\"))).pop(),\n",
    "        str(max_seq_length),\n",
    "    ),\n",
    ")\n",
    "b_dataset = get_balanced_dataset(dataset)\n",
    "torch.save({\"dataset\": b_dataset}, subset_cached_features_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d69f33",
   "metadata": {},
   "source": [
    "# Training\n",
    "- in GPU environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ae1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "cache_dir = '/content/cuad/data'\n",
    "model_name_or_path = 'roberta-base'\n",
    "max_seq_length=512\n",
    "evaluate = False\n",
    "subset_cached_features_file = os.path.join(\n",
    "    cache_dir,\n",
    "    \"balanced_subset_cached_{}_{}_{}\".format(\n",
    "        \"dev\" if evaluate else \"train\",\n",
    "        list(filter(None, model_name_or_path.split(\"/\"))).pop(),\n",
    "        str(max_seq_length),\n",
    "    ),\n",
    ")\n",
    "train_dataset = torch.load(subset_cached_features_file)[\"dataset\"]\n",
    "features, examples = None, None\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a74688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "cache_dir=None\n",
    "model_name_or_path = 'roberta-base'\n",
    "config_name = model_name_or_path\n",
    "config = AutoConfig.from_pretrained(\n",
    "    config_name,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "tokenizer_name = model_name_or_path\n",
    "do_lower_case=False\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_name,\n",
    "    do_lower_case=do_lower_case,\n",
    "    cache_dir=cache_dir,\n",
    "    use_fast=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb9c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/cuad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf20014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from train import build_args, train\n",
    "args = build_args(notebook=True)\n",
    "args.version_2_with_negative = True\n",
    "args.do_train=True\n",
    "# args.do_eval=True\n",
    "args.max_seq_length=512\n",
    "args.doc_stride=256\n",
    "args.max_query_length=128\n",
    "args.per_gpu_train_batch_size=8\n",
    "args.logging_steps=1000\n",
    "# args.per_gpu_eval_batch_size=32\n",
    "model.to(args.device)\n",
    "args.num_train_epochs = 4\n",
    "global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff33951",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '/content/cuad/train_models/roberta-base/'\n",
    "model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "torch.save(args, os.path.join(output_dir, \"training_args.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb4f26",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3fdea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.predict_file='/content/cuad/data/test_parties.json'\n",
    "args.version_2_with_negative=True\n",
    "from train import evaluate\n",
    "\n",
    "checkpoint = os.path.join(args.output_dir, 'checkpoint-2000')\n",
    "checkpoints = [checkpoint]\n",
    "# Reload the model\n",
    "global_step =  checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(checkpoint)  # , force_download=True)\n",
    "model.to(args.device)\n",
    "\n",
    "# Evaluate\n",
    "result = evaluate(args, model, tokenizer, prefix=global_step)\n",
    "\n",
    "result = dict((k + (\"_{}\".format(global_step) if global_step else \"\"), v) for k, v in result.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e60069",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8641dc82",
   "metadata": {},
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb11b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load_json, get_answers\n",
    "test_json_path = \"./data/test_parties.json\"\n",
    "model_path = \"./train_models/roberta-base\"\n",
    "save_dir = \"./results\"\n",
    "if not os.path.exists(save_dir): os.mkdir(save_dir)\n",
    "\n",
    "gt_dict = load_json(test_json_path)\n",
    "gt_dict = get_answers(gt_dict)\n",
    "\n",
    "predictions_path = os.path.join(model_path, \"nbest_predictions_.json\")\n",
    "name = model_path.split(\"/\")[-1]\n",
    "pred_dict = load_json(predictions_path)\n",
    "\n",
    "for k in list(pred_dict.keys()):\n",
    "    print(k)\n",
    "    print('GOLDs:', gt_dict[k])\n",
    "\n",
    "    for i, p in enumerate(pred_dict[k][:5], 1):\n",
    "        print(f'PRED@{i}:', p)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca99cc6",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275ebcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget -O CUAD_v1.zip https://zenodo.org/record/4595826/files/CUAD_v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUAD_v1.json       full_contract_pdf  label_group_xlsx\n",
    "# CUAD_v1_README.txt full_contract_txt  master_clauses.csv\n",
    "\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile('CUAD_v1.zip') as zfp:\n",
    "#     zfp.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c62ea0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "16728\n",
      "102\n",
      "4182\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# from collections import defaultdict\n",
    "# def squadv2_formatter(jd):\n",
    "#     data = jd['data']\n",
    "#     print(len(data))\n",
    "#     datasets = []\n",
    "#     for d in data:\n",
    "#         title = d['title']\n",
    "#         paragraph = d['paragraphs'][0]\n",
    "#         context = paragraph['context']\n",
    "#         qas = paragraph['qas']\n",
    "#         # 'is_impossible'\n",
    "#         q2id = defaultdict(list)\n",
    "#         q2text = defaultdict(list)\n",
    "#         q2answer_start = defaultdict(list)\n",
    "#         texts = []\n",
    "#         answer_starts = []\n",
    "#         for qa in qas:\n",
    "#             qid = qa['id']\n",
    "#             question = qa['question']\n",
    "#             for ad in qa['answers']:\n",
    "#                 text = ad['text']\n",
    "#                 answer_start = ad['answer_start']\n",
    "#                 texts.append(text)\n",
    "#                 answer_starts.append(answer_start)\n",
    "#             q2id[question].append(qid)\n",
    "#             q2text[question].extend(texts)\n",
    "#             q2answer_start[question].extend(answer_starts)\n",
    "#         for question in q2id:\n",
    "#             qid = q2id[question][0]\n",
    "#             datasets.append({\n",
    "#                 \"answers\": {\n",
    "#                     \"answer_start\": q2answer_start[question],\n",
    "#                     \"text\": q2text[question],\n",
    "#                 },\n",
    "#                 \"context\": context,\n",
    "#                 \"id\": qid,\n",
    "#                 \"question\": question,\n",
    "#                 \"title\": title\n",
    "#             })\n",
    "#     print(len(datasets))\n",
    "#     json_data = {\"version\": \"v2.0\", \"data\": datasets}\n",
    "#     return json_data\n",
    "\n",
    "\n",
    "# with open('./data/train_separate_questions.json') as reader:\n",
    "#     jd = json.load(reader)\n",
    "#     json_data = squadv2_formatter(jd)\n",
    "# with open('./data/train.json', 'w') as writer:\n",
    "#     writer.write(json.dumps(json_data))\n",
    "#     writer.write('\\n')\n",
    "# with open('./data/test.json') as reader:\n",
    "#     jd = json.load(reader)\n",
    "#     json_data = squadv2_formatter(jd)\n",
    "# with open('./data/test_new.json', 'w') as writer:\n",
    "#     writer.write(json.dumps(json_data))\n",
    "#     writer.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
